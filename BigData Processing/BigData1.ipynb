{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT 5148 - Distributed Databases and Big Data\n",
    "## Assignment 1\n",
    "_Due: Week 7 Monday 5PM_\n",
    "\n",
    "\n",
    "**Your details**\n",
    "- Name: Siyang Feng \n",
    "\n",
    "- Student ID: 28246993\n",
    "\n",
    "- Name: Xin Wen\n",
    "\n",
    "- Student ID: 28412702"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis:\n",
    "#### Data set characteristics: \n",
    "1. climate data is sorted by date from oldest to latest, because the data is collected by sensor, so it must be time sequential.\n",
    "2. Fire data is sorted by date from latest to oldest, because data is also collected by sensor.\n",
    "\n",
    "**We assume both tables are sorted by date**\n",
    "\n",
    "#### Table size:\n",
    "1. Climate - smaller -> when perform hash partition, we always use climate table to generate hash table\n",
    "2. Fire - larger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from copy import deepcopy as dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ClimateData.csv','r') as cliData:\n",
    "    cli = csv.reader(cliData, delimiter=',')\n",
    "    climate_data = list(cli)\n",
    "\n",
    "climate_data[:] = climate_data[1:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "climate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FireData.csv', 'r') as fireData:\n",
    "    fire_list = csv.reader(fireData)\n",
    "    fire_data = list(fire_list)\n",
    "\n",
    "fire_data[:] = fire_data[1:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fire_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check processor number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check processor number in system\n",
    "# used to determine the partition number\n",
    "processor_num = mp.cpu_count()\n",
    "processor_num # check processor number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partition number should be `processor_num - 1` at most. **\n",
    "\n",
    "The most efficient condition is when the number of parallel processors is 3. When it larger than 3, the increasing rate of efficiency decrease. \n",
    "\n",
    "However, in this task, we still consider the number of parallel processors is flexible. Because we think a good algorithm not only focus on its complexity but also suitable for more condition when its complexity doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define Partition number(processor number - 1)\n",
    "# because one of these processors is master to hold main function (parallel)\n",
    "parallel_num = processor_num - 1\n",
    "parallel_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global function for type converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = '%Y-%m-%d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting string into date\n",
    "def convert_string_to_date(dataset, dateIndex):\n",
    "    for i in dataset:\n",
    "        dateTime = datetime.datetime.strptime(i[dateIndex], date_format)\n",
    "        i[dateIndex] = dateTime.date()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting date into string\n",
    "def convert_date_to_string(dataset, dateIndex):\n",
    "    for i in dataset:\n",
    "        i[dateIndex] = i[dateIndex].strftime('%Y-%m-%d')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting string into integer\n",
    "def convert_string_to_int(dataset, index):\n",
    "    for i in dataset:\n",
    "        i[index] = int(i[index])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global function for determining range partiton indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_part_indices(dataset, n_proce, part_index):\n",
    "    \"\"\"\n",
    "    define the indices for range partition\n",
    "    \n",
    "    Arguments:\n",
    "    dataset -- input dataset\n",
    "    n_proce -- the number of processors used for parallel\n",
    "    part_index -- the position of partiton attribute in each sublist\n",
    "    \n",
    "    Return:\n",
    "    result -- a list of partition indices    \n",
    "    \"\"\"\n",
    "    #part_gap is the gap between each partition\n",
    "    part_gap = (dataset[-1][part_index] - dataset[0][part_index]) // n_proce \n",
    "    result = []\n",
    "    for i in range(1, n_proce):\n",
    "        result.append(dataset[0][part_index] + part_gap*i) # calculate the indice\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_partition(data, part_index, n_proce):\n",
    "    \"\"\"\n",
    "    Perform range data partition on data based on the partition attribute\n",
    "    \n",
    "    Arguments:\n",
    "    data -- an input dataset\n",
    "    range_list -- the list of the ranges to be split\n",
    "    part_index -- the position of partition attribute in each sublist\n",
    "    \n",
    "    Return:\n",
    "    result -- the partitioned subsets\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    # the data has been sorted\n",
    "    new_data = list(data)\n",
    "    range_list = range_part_indices(data, n_proce, part_index)\n",
    "    for i in range_list:\n",
    "        temp = [] # store the list of each partition\n",
    "        for j in range(len(new_data)):\n",
    "            if new_data[j][part_index] >= i: # this partition is created, iterate the next range indice\n",
    "                new_data = new_data[j:] \n",
    "                break\n",
    "            temp.append(new_data[j])\n",
    "        result.append(temp)\n",
    "    result.append(new_data)\n",
    "        \n",
    "    return result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global function for hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function(x, n):\n",
    "    \"\"\"\n",
    "    hash function to generate hash value\n",
    "    \n",
    "    Arguments:\n",
    "    x -- the value need to hash\n",
    "    n -- hash base value\n",
    "    \n",
    "    Result:\n",
    "    result -- hash value\n",
    "    \"\"\"\n",
    "    result = x%n\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_partition(data, n, index):\n",
    "    \"\"\"\n",
    "    hash function to partition dataset\n",
    "    \n",
    "    Arguements:\n",
    "    data -- a input dataset\n",
    "    n -- the number of partition\n",
    "    index -- the position of partition attribute in the dataset\n",
    "    \n",
    "    Return:\n",
    "    dic -- a dictionary of partitioned dataset\n",
    "    \"\"\"\n",
    "    dic = {} \n",
    "    for x in data: \n",
    "        h_data = x[index] # h_data is the attribute we are going to hash\n",
    "        h = hash_function(h_data,n) # h is the hash value of h_data\n",
    "        if (h in dic.keys()): # create hash table, key is h(hash value), value is the record of this value value.\n",
    "            dic[h].append(x) # if the key already exist, we append this record in the same entry.\n",
    "        else: \n",
    "            dic[h] = [x]  # if the key doesn't exist, we create one with the record.          \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_partition_date(data, n, dateIndex):\n",
    "    \"\"\"\n",
    "    a hash function for partitioning based on date\n",
    "    \n",
    "    Arguments:\n",
    "    data -- input dataset\n",
    "    n -- number of processors used for parallel\n",
    "    index -- the index of date in each item\n",
    "    \n",
    "    Return:\n",
    "    dic -- a dictionary of partition resault\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {} \n",
    "    for x in data: \n",
    "        h_data = x[dateIndex]\n",
    "        h_day = int(h_data[-2:]) # here we use the number of day in the date to do hash, and we need to convert it into integer first\n",
    "        h = hash_function(h_day,n)  # hash value of the date.\n",
    "        if (h in dic.keys()): \n",
    "            dic[h].append(x)\n",
    "        else: \n",
    "            dic[h] = [x]            \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Parallel Search\n",
    "#### Options for data partitioning methods:\n",
    "1. Round-robin\n",
    "    - Pro: load balancing\n",
    "    - Con: records are not grouped semantically, not efficient for searching, for each search, all processors need to be activated.\n",
    "    \n",
    "2. Hash\n",
    "    - Pro: based on attribute to do partition, if searching attribute is the same with hash attribute, then the performance is very good. (only need to activate one processor)\n",
    "    - Con: \n",
    "        * load imbalancing, skew\n",
    "        * If searching attribute is different from hash attribute, then not efficient.\n",
    "        * Not suitable for range searching, if user wants to do continuous range search, then all the processors need to be activated.\n",
    "    \n",
    "3. Range\n",
    "    - Pro: good for range searching (only need to activate one or selected processors)\n",
    "    - Con:\n",
    "        * Skew\n",
    "        * If searching attribute is different from range partition attribute, then not efficient.\n",
    "    \n",
    "4. Random-unequal\n",
    "    - Used for pipelining jobs and have skew problem\n",
    "\n",
    "#### Searching technique Options:\n",
    "1. Linear search:\n",
    "    * Suitable for unordered data\n",
    "2. Binary search\n",
    "    * Suitable for ordered data, more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "Write an algorithm to search climate data for the records on _**15th December 2017**_ . Justify your choice of the data partition technique and search technique you have used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision in task 1.1:\n",
    "we are doing exact match search, as climate data has one record for each day, so it is unique, if one match is found, the searching is completed.\n",
    "\n",
    "We assume the data is ordered by date(as it is from sensor), so range partition and hash partition are both possible, it is difficult to compare the cost of these two techniques, but hash partition will have to generate a hash table, and if the hash table size > main memory size, overflow problem can cause extra overhead.\n",
    "\n",
    "The disadvantage of the range partition technique is that skew will possibly occur, but we don't think round-robin is a good idea. It can achieve load balancing, but to do each search, all partitions need to be activated.\n",
    "\n",
    "So we decide to use range partition for task1.1, and use binary search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign climate data into another variable\n",
    "climate = dc(climate_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binary search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use binary_search for this task\n",
    "# because the data is already sorted by date\n",
    "# binary search is more efficent in this condition\n",
    "def binary_search_T1_1(data, key, key_index):\n",
    "    \"\"\"\n",
    "    binary search function to find key in dataset\n",
    "    \n",
    "    Arguments:\n",
    "    data -- input dataset\n",
    "    key -- target to find\n",
    "    key_index -- the index of key in each item of the dataset\n",
    "    \n",
    "    Return:\n",
    "    matched_record -- the matched item\n",
    "    \"\"\"\n",
    "    matched_record = None\n",
    "    lower = 0 \n",
    "    middle = 0 \n",
    "    upper = len(data)-1 \n",
    "    while(lower <= upper):\n",
    "        middle = int((lower + upper)/2)\n",
    "        x = data[middle]\n",
    "        if x[key_index] == key:\n",
    "            matched_record = x\n",
    "            break\n",
    "        elif x[key_index] > key:\n",
    "            upper = middle - 1\n",
    "        else:\n",
    "            lower = middle + 1\n",
    "    return matched_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing, use range_partition and binary_search to do this task\n",
    "# as there is only one record for each day in the climate data, \n",
    "# so if a match is found, the search is completed\n",
    "def parallel_search_exact_T1_1(data, query, date_index, n_processor):\n",
    "    \"\"\"\n",
    "    parallel processing to search a specific value\n",
    "    \n",
    "    Arguments:\n",
    "    data -- input dataset\n",
    "    query -- target to find\n",
    "    range_indices -- the list of range partition indices\n",
    "    date_index -- the index of key in each item of the dataset\n",
    "    n_processor -- the number of processor for parallel\n",
    "    \n",
    "    Return:\n",
    "    results -- find out item\n",
    "    \"\"\"\n",
    "    # convert string to date\n",
    "    data = convert_string_to_date(data, 1)\n",
    "    \n",
    "    results = []\n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "    DD = range_partition(data, date_index, n_processor)\n",
    "    for d in DD:\n",
    "        i = len(d)\n",
    "        last_i = i-1\n",
    "        if query > d[0][date_index] and query < d[i-1][date_index]:\n",
    "            result = pool.apply(binary_search_T1_1, [d,query,date_index])\n",
    "            results.append(result)\n",
    "            break\n",
    "    pool.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['948702',\n",
       "  datetime.date(2017, 12, 15),\n",
       "  '18',\n",
       "  '52',\n",
       "  '7.1',\n",
       "  '14',\n",
       "  '   74.5*',\n",
       "  '53.1',\n",
       "  ' 0.00I']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing\n",
    "q = datetime.datetime.strptime('2017-12-15',date_format).date() # the key for query\n",
    "results = parallel_search_exact_T1_1(climate, q, 1, parallel_num)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "Write an algorithm to find the latitude, longitude and confidence when the surface temperature (°C) was between 65 °C and 100 °C . Justify your choice of the data partition technique and search technique you have used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision in task 1.2:\n",
    "We are doing continuous range search, but the data is **unordered base on the searching attribute(surface temperature)**, so we decide to use hash partition for this task, as it doesn't need to sort the data first and quite efficient compared with random-equal partition.\n",
    "\n",
    "For the searching technique, we decide to use linear search, as it is an unordered dataset (based on the partition attribute).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire = dc(fire_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_search_T1_2(data, key, index):\n",
    "    \"\"\"\n",
    "    search target in linear\n",
    "    \n",
    "    Arguements:\n",
    "    data -- input dataset\n",
    "    key -- query item\n",
    "    index -- the position of the query key in each item of dataset(in this case, it is the index of surface temperature)\n",
    "    \n",
    "    Return:\n",
    "    result -- a list of matching items\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # loop data, this is not exact match, so we need to search all the data to find all matches.\n",
    "    for x in data:\n",
    "        if x[index] == key: ##if found, we append the record in the result\n",
    "            result.append(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiprocessing function, use hash_partition and linear_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_search_range_T1_2(data, query_range, n_processor, index):\n",
    "    \"\"\"\n",
    "    parallel rearch to search a range of key\n",
    "    \n",
    "    Arguements:\n",
    "    data -- input dataset\n",
    "    query_range -- a range of keys for searching\n",
    "    n_processor -- a number of processor for parallel\n",
    "    index -- the position of the query key in each item of dataset(in this case, it is the index of surface temperature)\n",
    "    \n",
    "    Return:\n",
    "    results -- a list of matching items\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "    data = convert_string_to_int(data, -1) #convert the surface temperature into integer\n",
    "    \n",
    "    DD=hash_partition(data, n_processor, index) #do hash partition on the dataset, DD is the partitioned dataset.\n",
    "    \n",
    "    for x in range(query_range[0],query_range[1]+1): # the query range is continuous, so we need to search for each value in between.\n",
    "        query_hash = hash_function(x,n_processor) # hash the value in search range first, query_hash is the hashed value.\n",
    "        d = list(DD[query_hash]) #d is the records in the hash partition of query_hash.\n",
    "        result = pool.apply(linear_search_T1_2, [d, x, index])# do parallel linear_search for all values in the search range\n",
    "        if len(result) > 0: #only append result which has found match.\n",
    "            results.append(result)\n",
    "    pool.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "q = [65,100]  # the searching query\n",
    "result = parallel_search_range_T1_2(fire, q, parallel_num, -1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Parallel Join\n",
    "#### Join algorithm options:\n",
    "1. Nested loop\n",
    "    - Not efficient, records are scanned more than once\n",
    "2. Sort-merge join\n",
    "    - Each record needs to be scanned only once\n",
    "    - Data needs to be sorted first.\n",
    "3. Hash-based join\n",
    "    - Each record needs to be scanned only once\n",
    "    - Data needs to be hashed first(create a hash table)\n",
    "\n",
    "#### Parallel algorithm for join:\n",
    "1. Divide and broadcast\n",
    "    - Divide the larger table using round-robin, then broadcast the smaller table to all processors\n",
    "    - Local join can use any join algorithm\n",
    "    - Pro: \n",
    "        * load balancing\n",
    "    - Con: \n",
    "        * skew for result production, smaller table is duplicated\n",
    "2. Disjoint data partitioning\n",
    "    - Data partitioning need to use disjoint partition(range or hash)\n",
    "    - Local join can use any serial algorithm\n",
    "    - Pro:\n",
    "        * No duplication of smaller table\n",
    "        * If use range partition, table needs to be sorted first\n",
    "        * If use hash partition, needs to create a hash table first\n",
    "    - Con:\n",
    "        * Skew\n",
    "\n",
    "#### Decision:\n",
    "Task 2.1 and 2.2 will join two tables based on the Date attribute, although tables are sorted by date, it is very difficult to tell the date range of these two tables before looking through the data (because two tables could have different date range, which makes it complex to determine range indices).\n",
    "\n",
    "For these 2 subtasks, we prefer to use hash partition and hash-based join.\n",
    "\n",
    "* For the data partition, we use hash partition based on join attribute(date) and \n",
    "* For the local join, we use hash-based join.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "Write an algorithm to find 1)surface temperature (°C), 2)air temperature (°C), 3)relative humidity and 4)maximum wind speed. Justify your choice of the data partition technique and join technique you have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign datasets of fire and climate data\n",
    "climate = dc(climate_data)\n",
    "fire = dc(fire_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hash based join function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HB_join_T2_1(T1, T2):\n",
    "\n",
    "    \"\"\" \n",
    "    Perform the hash-based join algorithm.\n",
    "\n",
    "    The join attribute is the date attribute in the input tables T1 & T 2\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    T1 - fire - larger table (used for probing)\n",
    "    \n",
    "\n",
    "    Return:\n",
    "\n",
    "    result -- the joined result\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "\n",
    "    dic = {} # hash table\n",
    "\n",
    "    # For each record in table T2 climate, create local hash table\n",
    "    for s in T2:\n",
    "        \n",
    "        #use the day value of the date(index 1) to do hash function\n",
    "        s_date = s[1] \n",
    "        s_day = int(s_date[-2:])\n",
    "        \n",
    "        # Hash the record based on join attribute value using hash function\n",
    "        # the hash table has to be different from global hash table, we choose 6 to generate local hash table\n",
    "        s_key = hash_function(s_day, 6)  \n",
    "        if s_key in dic:\n",
    "            dic[s_key].append(s) # If there is an entry \n",
    "        else:\n",
    "            dic[s_key] = [s]\n",
    "\n",
    "    # For each record in table T1 fire (probing) \n",
    "    for r in T1:\n",
    "    # Hash the record based on join attribute value using hash function\n",
    "        r_date = r[-2] # -2 is the index of date in table T1\n",
    "        r_day = int(r_date[-2:])\n",
    "        r_key = hash_function(r_day, 6) \n",
    "\n",
    "    # If an index entry is found Then \n",
    "        # Compare each record on this index entry with the record of table T2\n",
    "        # If the key is the same then put the result \n",
    "        if r_key in dic:\n",
    "            for i in dic[r_key]:\n",
    "                if r[-2] == i[1]: # r is record in T1 (fire), i is entry in hash table (based on T2, climate)\n",
    "                    result.append({\", \".join([r[-1], i[2], i[3], i[5]])}) \n",
    "                    #r[-1] surface temperature, i[2] air temperature, i[3] relative humidity, i[5] maximum wind speed\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiprocessing using disjoint partition-based parallel join，this function is used for task2-1 and task2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DPBP_join_T2(T1, T2, n_processor, T1_partIndex, T2_partIndex, HB_join):\n",
    "    \"\"\"\n",
    "    disjoint partition-based parallel join function\n",
    "    \n",
    "    Arguments:\n",
    "    T1 -- input dataset (fire) - larger table\n",
    "    T2 -- input dataset (climate) - smaller table\n",
    "    n_processor -- number of processor for parallel\n",
    "    T1_partIndex -- date index in each item of fire dataset\n",
    "    T2_partIndex -- date index in each item of climate dataset\n",
    "    HB_join -- hash-based join function to use\n",
    "    \n",
    "    Return:\n",
    "    result -- the joined list\n",
    "    \"\"\"\n",
    "    \n",
    "    result = [] \n",
    "    \n",
    "    # perform hash partition for both tables\n",
    "    T1_subsets = hash_partition_date(T1, n_processor, T1_partIndex)\n",
    "\n",
    "    T2_subsets = hash_partition_date(T2, n_processor, T2_partIndex)\n",
    "    \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    \n",
    "    # Apply local join for each processor\n",
    "    for index in range(n_processor):\n",
    "        result.append(pool.apply(HB_join, [T1_subsets[index], T2_subsets[index]]))\n",
    "    pool.close()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "result = DPBP_join_T2(fire, climate, processor_num, -2, 1, HB_join_T2_1)\n",
    "#surface temperature, air temperature, relative humidity, maximum wind speed\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "Write an algorithm to find 1)datetime, 2)air temperature (°C), 3)surface temperature (°C) and 4)confidence when the confidence is between 80 and 100. Justify your choice of the data partition technique and join technique you have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign datasets of fire and climate data\n",
    "climate = dc(climate_data)\n",
    "fire = dc(fire_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hash based join function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HB_join_T2_2(T1, T2):\n",
    "\n",
    "    \"\"\" \n",
    "    Perform the hash-based join algorithm.\n",
    "\n",
    "    The join attribute is the date attribute in the input tables T1 & T 2\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    T1 - fire - larger table (used for probing)\n",
    "    \n",
    "\n",
    "    Return:\n",
    "\n",
    "    result -- the joined result\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "\n",
    "    dic = {} # hash table\n",
    "\n",
    "    # For each record in table T2 climate, create local hash table\n",
    "    for s in T2:\n",
    "        \n",
    "        #use the day value of the date(index 1) to do hash function\n",
    "        s_date = s[1] \n",
    "        s_day = int(s_date[-2:])\n",
    "        \n",
    "        # Hash the record based on join attribute value using hash function\n",
    "        # the hash table has to be different from global hash table, we choose 6 to generate local hash table\n",
    "        s_key = hash_function(s_day, 6)  \n",
    "        if s_key in dic:\n",
    "            dic[s_key].append(s) # If there is an entry \n",
    "        else:\n",
    "            dic[s_key] = [s]\n",
    "\n",
    "    # For each record in table T1 fire (probing) \n",
    "    for r in T1:\n",
    "    # Hash the record based on join attribute value using hash function\n",
    "        r[5] = int(r[5]) # convert confidence into integer for further comparison\n",
    "        r_date = r[-2] # -2 is the index of date in table T1\n",
    "        r_day = int(r_date[-2:])\n",
    "        r_key = hash_function(r_day, 6) \n",
    "\n",
    "    # If an index entry is found Then \n",
    "        # Compare each record on this index entry with the record of table T2\n",
    "        # If the key is the same then put the result \n",
    "        if r_key in dic:\n",
    "            for i in dic[r_key]:\n",
    "                # r is record in T1 (fire), i is entry in hash table (based on T2, climate)\n",
    "                # only find record with confident between 80 and 100\n",
    "                if r[-2] == i[1] and r[5] >= 80 and r[5] <= 100:                    \n",
    "                    result.append({\", \".join([r[3], i[2], r[-1], str(r[5])])}) \n",
    "                    #r[3] datetime, i[2] air temperature, r[-1] surface temperature, r[5] confidence\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "result = DPBP_join_T2(fire, climate, parallel_num, -2, 1, HB_join_T2_2)\n",
    "#datetime, air temperature, surface temperature, confidence\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Parallel Sort\n",
    "#### Options:\n",
    "1. Parallel merge-all sort\n",
    "    - Pro: load balancing\n",
    "    - Con: \n",
    "        * merge heavy in one processor(host)\n",
    "        * Network contention\n",
    "        * If the number of files open at the same time is limited, this method cannot be used.\n",
    "    - Decision: \n",
    "        * not use this technique, because merge phase will be the bottleneck\n",
    "\n",
    "2. Parallel binary-merge sort\n",
    "    - Pro: \n",
    "        * workload of merging is spread through pipeline of processors\n",
    "        * Sort phase can achieve load balancing by using round-robin\n",
    "    - Con:\n",
    "        * For the host, still heavy work in the final merge phase\n",
    "        * Extra work and higher tree is caused by using pipeline\n",
    "    - Decision: \n",
    "        * not use this method, because for the host, still heavy work, not efficient.\n",
    "\n",
    "3. Parallel redistribution merge-all sort\n",
    "    - Pro:\n",
    "        * One level tree\n",
    "        * Merging stage use parallelism\n",
    "        * Merge work is lighter compared with no redistribution.\n",
    "\n",
    "    - Con:\n",
    "        * In the merge stage, skew may occur\n",
    "        * File opening at the same time can be the bottleneck at final stage\n",
    "\n",
    "    - Decision: \n",
    "        * this method is good, parallelism can be achieved and the tree is only one level\n",
    "\t\n",
    "4. Parallel redistribution binary-merge sort\n",
    "    - Pro:\n",
    "        * Merging stage use parallelism, and parallelism is achieved at all levels of merging\n",
    "        * Merge work is lighter compared with no redistribution.\n",
    "    - Con:\n",
    "        * The tree is high because of the pipeline\n",
    "        * Skew may occur in the merging stage\n",
    "        * Final merging stage will also have the file opening bottleneck\n",
    "\n",
    "    - Decision: \n",
    "        * not use this method, because the tree is high.\n",
    "\t\n",
    "5. Parallel Partitioned sort\n",
    "    - Pro:\n",
    "        * No merging actually happens, so there’s no bottleneck\n",
    "        * Parallelism is achieved\n",
    "        * One level tree\n",
    "    - Con:\n",
    "        * Skew of work\n",
    "    - Decision: \n",
    "        * this method is good, the only problem is that we need to decide the range indices, so we need to scan the data first, as the data is not ordered by the sort attribute, which can be slow.\n",
    "\n",
    "#### Summary:\n",
    "After comparing these techniques and based on the dataset, we decide to use round-robin for partition(load balancing) and use parallel redistribution merge-all sort (parallelism of sorting and merging, less I/O, one-level tree).\n",
    "\n",
    "And we choose quick sort for the local sort technique, because it is more efficient than bubble sort and insertion sort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task:\n",
    "Write an algorithm to sort ​ fire data ​ based on ​ surface temperature ​ ​ (°C) ​ in a ​ ascending order. ​ Justify your choice of the data partition technique and sorting technique you have used\n",
    "\n",
    "\n",
    "__Parallel redistribution mrege-all sort:__\n",
    "\n",
    "* _First distribute:_ Round-Robin\n",
    "* _Range Distribute:_ compare the first item of each partition and last item of each partition get the min and max value and define the partition range. Then, distribute in each process.\n",
    "* _Merge all sort:_ In each local partition, use merge all to merge sort each partition (k-way-merge)\n",
    "* _Merge as range_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign datasets of fire data\n",
    "fire = dc(fire_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-robin data partitioning function\n",
    "def rr_partition_T3(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "    \n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "    \n",
    "    Return:\n",
    "    result -- the partitioned subsets of data\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    # partition the data into each bin\n",
    "    for index, item in enumerate(data):\n",
    "        # use the index to calculate the partitioning bin\n",
    "        result[(int)(index % n)].append(item)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qsort_T3(arr): \n",
    "\n",
    "    \"\"\" \n",
    "    Quicksort a list\n",
    "    \n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted list\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        return qsort_T3([x for x in arr[1:] if int(x[-1]) < int(arr[0][-1])]) \\\n",
    "                + [arr[0]] \\\n",
    "                + qsort_T3([x for x in arr[1:] if int(x[-1]) >= int(arr[0][-1])])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_range_T3(list_sum, n_proc):\n",
    "    \"\"\"\n",
    "    Create the range indices list for next step to do range partition.\n",
    "    Find out the max value and min value in each subset and compare them to find the max and min for the whole dataset. \n",
    "    The partition point can be divided by their range between max and min and \n",
    "    the number of processors.\n",
    "    \n",
    "    Arguments:\n",
    "    list_sum -- a list of sorted subset\n",
    "    n_proc -- the number of proccess using in parallel proccessing\n",
    "    \n",
    "    Return:\n",
    "    range_list -- the range indices for range_partition\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the max and min temperature in the first subset\n",
    "    min_v = int(list_sum[0][0][-1]) #-1 is the index of surface temperature\n",
    "    max_v = int(list_sum[0][-1][-1])\n",
    "    \n",
    "    # compare the max and min in other subset with the first subset's min and max, find the max and min for the whole dataset\n",
    "    for item in list_sum[1:]:\n",
    "        if int(item[0][-1]) < min_v:\n",
    "            min_v = int(item[0][-1])\n",
    "        if int(item[-1][-1]) > max_v:\n",
    "            max_v = int(item[-1][-1])\n",
    "    \n",
    "    # determine the range indice for the next step\n",
    "    range_v = int((max_v - min_v) / n_proc)\n",
    "    range_list = list()\n",
    "    for i in range(1, n_proc):\n",
    "        range_list.append(min_v + i*range_v)\n",
    "    return range_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_partition_T3(dataset, range_list):\n",
    "    \"\"\"\n",
    "    partition the sorted dataset\n",
    "    \n",
    "    Arguements:\n",
    "    dataset -- sorted data list\n",
    "    range_list -- list of the range indices\n",
    "    \n",
    "    Return:\n",
    "    result -- list of partitioned dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # the data has been sorted\n",
    "    for i in range_list:\n",
    "        temp = []\n",
    "        for j in range(len(dataset)):\n",
    "            if int(dataset[j][-1]) > i:\n",
    "                dataset = dataset[j:]\n",
    "                break\n",
    "            temp.append(dataset[j])\n",
    "        result.append(temp)\n",
    "    result.append(dataset)\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the smallest record\n",
    "def find_min_T3(records):\n",
    "    \"\"\"\n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguements:\n",
    "    records -- the input record set\n",
    "    \n",
    "    Return :\n",
    "    index -- the smallest record's index\n",
    "    \"\"\"\n",
    "    \n",
    "    m = int(records[0][-1])\n",
    "    index = 0\n",
    "    for i in range(1, len(records)):\n",
    "        if(int(records[i][-1]) < m):\n",
    "            index = i\n",
    "            m = int(records[i][-1])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_way_merge_T3(record_sets, buffer_size):\n",
    "    \"\"\"\n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of multiple sorted sub-record sets\n",
    "    \n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ##check buffer_size first\n",
    "    if (buffer_size <= 2):\n",
    "\n",
    "        print(\"Error: buffer size should be greater than 2\") \n",
    "        return #end of method.\n",
    "    \n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for x in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "        \n",
    "    # final result will be sorted in this variable\n",
    "    result = []\n",
    "    \n",
    "    # the merging unit (i.e. # of the given buffers)\n",
    "    tuple = []\n",
    "    \n",
    "    while True:\n",
    "        tuple = [] # initialise tuple\n",
    "        \n",
    "        # Loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if (indexes[i] >= len(record_sets[i])):\n",
    "                tuple.append([sys.maxsize])\n",
    "            else:\n",
    "                tuple.append(record_sets[i][indexes[i]])\n",
    "        \n",
    "        # find the smallest record\n",
    "        smallest = find_min_T3(tuple)\n",
    "        \n",
    "        # if there is only sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if int(tuple[smallest][-1]) == sys.maxsize:\n",
    "            break\n",
    "            \n",
    "        # this record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] += 1\n",
    "        \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_merge_T3(sorted_lists):\n",
    "    \"\"\"\n",
    "    merge the sorted lists together by sorting\n",
    "    \n",
    "    Arguments:\n",
    "    sorted_lists -- a list with sorted sublists\n",
    "    \n",
    "    Return:\n",
    "    result -- a list merged sublists in ascending order\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(sorted_lists)-1):\n",
    "        for j in range(i+1, len(sorted_lists)):\n",
    "            if int(sorted_lists[j][0][-1]) < int(sorted_lists[i][0][-1]):\n",
    "                sorted_lists[i], sorted_lists[j] = sorted_lists[j], sorted_lists[i]\n",
    "    \n",
    "    result = list(itertools.chain(*sorted_lists)) #create the result list\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_merge_sort_T3(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a perellal k-merge sort method\n",
    "    \n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- the number of processors\n",
    "    \n",
    "    Return:\n",
    "    result -- the sorted result\n",
    "    \"\"\"\n",
    "    \n",
    "    ##check buffer_size first\n",
    "    if (buffer_size <= 2):\n",
    "\n",
    "        print(\"Error: buffer size should be greater than 2\") \n",
    "        return #end of method.\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # partition dataset through round-robin\n",
    "    rr_list = rr_partition_T3(dataset, n_processor)\n",
    "    \n",
    "    \n",
    "    # define the parallel processor number\n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "    \n",
    "    \n",
    "    # parallel sort items in partition dataset, rr_list, by quick sort\n",
    "    qsort_list = []\n",
    "    for i in rr_list:\n",
    "        qsort_list.append(pool.apply(qsort_T3, [i]))\n",
    "        \n",
    "    pool.close()\n",
    "    \n",
    "    \n",
    "    # get the list of partition points for further range partition\n",
    "    range_list = distribute_range_T3(qsort_list, n_processor)\n",
    "    \n",
    "    # create a partition list with sublists which is defined by processor number\n",
    "    partition_list = []\n",
    "    for i in range(n_processor):\n",
    "        partition_list.append([])\n",
    "    \n",
    "    # because each partition will contain sublists which is divided by partition points defined before\n",
    "    # for next parallel processing, lists in same range should merge into a same list\n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "    for i in qsort_list:\n",
    "        record = pool.apply(range_partition_T3, [i, range_list])\n",
    "        for j, item in enumerate(record):\n",
    "            partition_list[j].append(item)\n",
    "    pool.close()\n",
    "    \n",
    "    # do local k-way-merge\n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "    merged_list = []\n",
    "    for i in partition_list:\n",
    "        merged_list.append(pool.apply(k_way_merge_T3, [i, buffer_size]))  # k-way merge sort to merge each sublist together\n",
    "        \n",
    "    pool.close()\n",
    "    \n",
    "    result = range_merge_T3(merged_list)  # merge sublist by sorting their first item\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Testing\n",
    "parallel_merge_sort_T3(fire, parallel_num, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 Paralle Groupby\n",
    "##### Technique comparison:\n",
    "1. Merge-all groupby\n",
    "    - Pro:\n",
    "        * Suitable when the number of nodes are small and the number of resulting records are small\n",
    "    - Con:\n",
    "        * If the group size is bigger, host will become the bottleneck\n",
    "2. Hierarchical merging groupby\n",
    "    - Pro:\n",
    "        * Suitable when the number of resulting records are small\n",
    "    - Con:\n",
    "        * If the group size is bigger, host will still be the bottleneck\n",
    "\n",
    "3. Two-phase method:\n",
    "    - Pro:\n",
    "        * Distribute local results based on the group-by attribute, this solved the bottleneck problem\n",
    "\n",
    "4. Redistribution method\n",
    "    - Pro:\n",
    "        * No need to do local aggregation before distribution, more efficient\n",
    "        * No bottleneck problem\n",
    "\n",
    "##### Examine data characteristic\n",
    "1. Fire data is sorted by date, so it is easy for us to do range partition (easy to determine range indices)\f",
    "\n",
    "\n",
    "##### Decision:\n",
    "- Based on the dataset characteristic and pros/cons of each technique, our group decide to use **redistribution method** for this task (range partition), as it is more efficient, and avoid the bottleneck problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1\n",
    "Write an algorithm to get the number of fire in each day. \n",
    "\n",
    "You are required to only display total number of fire and the date in the output. \n",
    "\n",
    "Justify your choice of the data partition technique if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign datasets of fire and climate data\n",
    "fire = dc(fire_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### local groupby function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_groupby_count(dataset, gb_index):\n",
    "    \"\"\" \n",
    "    a local groupby function to calculate the number of records\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    dataset -- entire record set to be merged\n",
    "    gb_index -- the index of groupby attribute in each item\n",
    "\n",
    "    Return:\n",
    "\n",
    "    result -- the aggregated record set according to the group_by attribute index \n",
    "    \"\"\"\n",
    "    \n",
    "    # convert date to string first because we need to use string to create dictionary key\n",
    "    dataset = convert_date_to_string(dataset, gb_index) \n",
    "\n",
    "    dict = {} \n",
    "    for index, record in enumerate(dataset):\n",
    "        key = record[gb_index] \n",
    "        if key not in dict:\n",
    "            dict[key] = 1  \n",
    "        else:\n",
    "            dict[key] += 1\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_redistribution_groupby_T4_1(dataset, date_index, n_processor):\n",
    "\n",
    "    \"\"\" Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    dataset -- entire record set to be merged\n",
    "    date_index -- the index of groupby attribute\n",
    "    n_processor -- the number of partitions\n",
    "\n",
    "    Return:\n",
    "\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index \n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    # convert string into date first to perform range_partition\n",
    "    \n",
    "    dataset = convert_string_to_date(dataset, date_index) \n",
    "    \n",
    "    # perform range_partition based on date\n",
    "    \n",
    "    DD = range_partition(dataset, date_index, n_processor)\n",
    "\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Local aggregation step ----\n",
    "    for d in DD:\n",
    "\n",
    "        # call the local aggregation method\n",
    "\n",
    "        result.append(pool.apply(local_groupby_count, [d, date_index])) \n",
    "        \n",
    "    pool.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fire data is ordered from latest date to oldest, so we need to reverse it first.\n",
    "result = parallel_redistribution_groupby_T4_1(list(reversed(fire)), -2, parallel_num)\n",
    "# date, total number of fire in that day\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2\n",
    "Write an algorithm to find the average surface temperature(°C) for each day\n",
    "\n",
    "You are required to only display average surface temperature (°C) and the date in the output.\n",
    "\n",
    "Justify your choice of the data partition technique if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire = dc(fire_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### local groupby function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_groupby_average(dataset, gb_index):\n",
    "\n",
    "    \"\"\" \n",
    "    a local groupby function for calculating average\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    dataset -- entire record set to be merged\n",
    "    gb_index -- the index of groupby attribute in each item\n",
    "\n",
    "    Return:\n",
    "\n",
    "    result -- the aggregated record set according to the group_by attribute index \n",
    "    \"\"\"\n",
    "\n",
    "    # convert date to string first because we need to use string to create dictionary key\n",
    "    dataset = convert_date_to_string(dataset, gb_index) \n",
    "\n",
    "    dict = {} \n",
    "    for index, record in enumerate(dataset):\n",
    "        key = record[gb_index] \n",
    "        if key not in dict:\n",
    "            # 1 is the number of record, record[-1] is the index of column we need to calculate average, we save these two value to get average\n",
    "            dict[key] = [1, record[-1]]\n",
    "        else:\n",
    "            dict[key][0] += 1 # add the record number by 1\n",
    "            dict[key][1] += record[-1] # add the value to sum\n",
    "    for i in dict:\n",
    "        avg = round(dict[i][1]/dict[i][0],2) # calculate the average and round it.\n",
    "        dict[i] = avg #save the average value into dictionary\n",
    "    \n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_redistribution_groupby_T4_2(dataset, date_index, n_processor):\n",
    "\n",
    "    \"\"\" Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index \n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    #convert string into date (column date, index -2)\n",
    "    dataset = convert_string_to_date(dataset,-2)\n",
    "    #convert string into int (column surface temperature, index -1)\n",
    "    dataset = convert_string_to_int(dataset, -1)\n",
    "    \n",
    "    DD = range_partition(dataset, date_index, n_processor)\n",
    "\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Local aggregation step ----\n",
    "    for d in DD:\n",
    "\n",
    "        # call the local aggregation method\n",
    "\n",
    "        result.append(pool.apply(local_groupby_average, [d, date_index])) \n",
    "        \n",
    "    pool.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fire data is ordered from latest date to oldest, so we need to reverse it first.\n",
    "result = parallel_redistribution_groupby_T4_2(list(reversed(fire)), -2, parallel_num)\n",
    "# date, average surface temperature in that day\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 Group-by join\n",
    "\n",
    "Write an algorithm to find the average surface temperature (°C) for each weather station.\n",
    "\n",
    "You are required to only display average surface temperature (°C) and the station in the output.\n",
    "\n",
    "Justify your choice of the data partition and join technique.\n",
    "\n",
    "Hint: You need to join using the date and group by based on station.\n",
    "\n",
    "__Groupby attribute__ is weather station\n",
    "\n",
    "__Join attribute__ is date\n",
    "- > we choose to use groupby after join, because groupby attribute is different from join attribute\n",
    "\n",
    "__Textbook reference:__ \n",
    "For efﬁciency, when the join selectivity factor is small and the degree of skewness is low, the join partitioning scheme leads to less cost; otherwise, the GroupBy partitioning scheme is desirable. In addition, it can be observed that the partitioning with the group-by attribute scheme is insensitive to the group-by factor and thus the scheme will simplify algorithm design and implementation.\n",
    "\n",
    "#### Determine the partition attribute：\n",
    "##### Choice1: use join attribute to do partition\n",
    "- Pro: \n",
    "    * no need to broadcast the fire table\n",
    "    * Less cost when the join selectivity factor is small, and degree of skewness is low\n",
    "- Con:\n",
    "    * Algorithm is more complex\n",
    "    \n",
    "##### Choice2: use groupby attribute to do partition\n",
    "- Pro:\n",
    "    * Less cost when the join selectivity factor is large, and degree of skewness is high\n",
    "    * insensitive to the group-by factor and algorithm design and implementation is simpler\n",
    "\n",
    "##### Summary:\n",
    "For this task, we choose to use groupby attribute, because the join selectivity ratio is high, and the algorithm is simpler.\n",
    "\n",
    "For the data partitioning technique, we choose to use hash partition,\n",
    "\n",
    "For the local join, we choose to use hash-based join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign datasets of fire and climate data\n",
    "fire = dc(fire_data)\n",
    "climate = dc(climate_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  hash based join for local join\n",
    "join each climate partition with the whole fire table, this is shared memory, so we don't need to broadcast it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HB_join_T5(T1, T2, n_processor):\n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    Perform the hash-based join algorithm.\n",
    "\n",
    "    The join attribute is the date attribute in the input tables T1 & T 2\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    T1 - fire - larger table (probing)\n",
    "    T2 - climate - smaller table (create hash table)\n",
    "    \n",
    "\n",
    "    Return:\n",
    "\n",
    "    result -- the joined result\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "\n",
    "    dic = {}\n",
    "\n",
    "    # For each record in table T2 （climate)\n",
    "    for s in T2:\n",
    "        s_date = s[1] # 1 is the index of date attribute in table T2\n",
    "        s_day = int(s_date[-2:])\n",
    "        # Hash the record based on join attribute value using hash function into hash table\n",
    "        \n",
    "        s_key = hash_function(s_day, n_processor) \n",
    "        if s_key in dic:\n",
    "            dic[s_key].append(s)\n",
    "        else:\n",
    "            dic[s_key] = [s]\n",
    "\n",
    "    # For each record in table T1 (fire) (probing) \n",
    "    # Hash the record based on join attribute value using hash function\n",
    "    for r in T1:\n",
    "        r_date = r[-2] # -2 is the index of date attribute in table T1\n",
    "        r_day = int(r_date[-2:])\n",
    "        r_key = hash_function(r_day, n_processor)\n",
    "\n",
    "    # If an index entry is found Then \n",
    "        # Compare each record on this index entry with the record of table T1\n",
    "        # If the key is the same then put the result \n",
    "        if r_key in dic:\n",
    "            for i in dic[r_key]:\n",
    "                if r[-2] == i[1]:\n",
    "                    # i[0] station number, r[-1] average surface temperature\n",
    "                    result.append([str(i[0]), r[-1]])\n",
    "                    \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiprocessing for parallel join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DPBP_join_T5(T1, T2, n_processor, T2_partIndex):\n",
    "    \n",
    "    \"\"\"\n",
    "    disjoint partition-based parallel join function\n",
    "    \n",
    "    Arguments:\n",
    "    T1 -- input dataset (fire) - larger table\n",
    "    T2 -- input dataset (climate) - smaller table\n",
    "    n_processor -- number of processor for parallel\n",
    "    partIndex -- date index in each item of climate dataset (T2\n",
    "    \n",
    "    Return:\n",
    "    result -- the joined list\n",
    "    \"\"\"\n",
    "    \n",
    "    result = [] \n",
    "    \n",
    "    #do partition on the climate table, the partition attribute is station.\n",
    "    T2_subsets = hash_partition(T2, n_processor, T2_partIndex)\n",
    "    \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    # Apply local join for each processor\n",
    "    for index in range(n_processor):\n",
    "        result.append(pool.apply(HB_join_T5, [T1, T2_subsets[index], n_processor]))\n",
    "    \n",
    "    pool.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### local groupby function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_groupby_T5(dataset):\n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    Perform a local groupby method to get average value\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "\n",
    "    result -- the aggregated record set according to the group_by attribute index \n",
    "    \"\"\"\n",
    "    dict = {} \n",
    "    for i in dataset:\n",
    "        key = i[0] \n",
    "        if key not in dict:\n",
    "            dict[key] = [1, int(i[1])]\n",
    "        else:\n",
    "            dict[key][0] += 1\n",
    "            dict[key][1] += int(i[1])\n",
    "    for i in dict:\n",
    "        avg = round(dict[i][1]/dict[i][0],2)\n",
    "        dict[i] = avg\n",
    "    \n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiprocessing for parallel groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_groupby_T5(T1, T2, n_processor, partIndex):\n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    \n",
    "    T1 --\n",
    "\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index \n",
    "    \"\"\"\n",
    "    # convert station number into int (climate table)\n",
    "    T2 = convert_string_to_int(T2, 0)\n",
    "    \n",
    "    \n",
    "    #join two tables first\n",
    "    dataset = DPBP_join_T5(T1, T2, n_processor, 0)\n",
    "    \n",
    "\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Local aggregation step ----\n",
    "    result = [] \n",
    "    for s in dataset:\n",
    "        \n",
    "        # call the local aggregation method\n",
    "\n",
    "        result.append(pool.apply(local_groupby_T5, [s])) \n",
    "    pool.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'948702': 52.15}, {}, {'948701': 56.07}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing\n",
    "result = parallel_groupby_T5(fire, climate, parallel_num, 0)\n",
    "# station number, average surface temperature (°C)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
