{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Generate Sparse Representations \n",
    "#### Student Name: Siyang Feng\n",
    "#### Student ID: 28246993\n",
    "\n",
    "Date: 04/06/2018\n",
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.6.2 and Anaconda 4.3.29\n",
    "\n",
    "Libraries used:\n",
    "\n",
    "* re 2.2.1 (for regular express, included in Anaconda Python 3.6)\n",
    "* os (for getting files in directory, included in Anaconda Python 3.6)\n",
    "* nltk (for nature language pre-processing)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "The aim of this task is to build sparse representations for the meeting transcripts generated in task 1, which includes word tokenization, vocabulary generation, and the generation of sparse representations. \n",
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text file\n",
    "* the sample output text has been deleted. Thus, we should just read all text files.\n",
    "\n",
    "All the text file is recorded into a dictionary with the key of file name without '.txt' and value of string.\n",
    "\n",
    "Text file path and target sparse file path is record in two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text path\n",
    "txt_path = \"./txt_files\"\n",
    "sparse_path = \"./sparse_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `parse_one_text` is used to read a text file and get the contains as a string.\n",
    "\n",
    "The next step is used to generate a dictionary to store all the contains in all text files in `./txt_files`. The key of the dictionary is the text name without '.txt' and the value is a string storing all the contains in this text file. Notice that, all the string values are converted into lower case in the dictionary. The dictionary result would be:\n",
    "``` json\n",
    "{\n",
    "    'ES2002a': \" okay .\\n right .\\n um well this is the kick-off meeting for our our project . ...\"\n",
    "    'ES2002b': \" is that alright now ?\\n okay .\\n sorry ?\\n okay , ...\"\n",
    "    ...\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_text(f_path):\n",
    "    \"\"\"\n",
    "    Read a text file with a defined text path t.\n",
    "    Parse the text file in to a string.\n",
    "    \n",
    "    Arguments:\n",
    "    f_path -- the text file path\n",
    "    \n",
    "    Return:\n",
    "    t -- the string record all the contains in text file\n",
    "    \"\"\"\n",
    "    r = open(f_path, 'r')\n",
    "    t = r.read()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the text file in directory 'txt_files' and load into a dict with the key of its file name\n",
    "text_dict = dict()\n",
    "for tfile in os.listdir(txt_path):\n",
    "    t_path = os.path.join(txt_path, tfile)\n",
    "    if os.path.isfile(t_path) and t_path.endswith('.txt'):\n",
    "        text_name= re.search(r'(.+).txt', tfile).group(1)\n",
    "        text_dict[text_name] = parse_one_text(t_path).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the words\n",
    "The text has been extracted. Then, we need tokenize all the words with regular expression tokenizer implemented in NLTK.\n",
    "The token pattern is `\\w+(?:[-']\\w+)?`. Use this pattern with the function `RegexpTokenizer()` to get the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `tokenize_words()` is used to tokenize all the tokens in every text file. All these topic file are recorded into a dictionary which is generated before. The input of this function will be the text file dictionary. The tokenizer will be used in the values of the text dictionary. The output of this function is a new dictionary which record the file name as key and the original tokens in each file as value of the corresponding key. The structure would like:\n",
    "``` json\n",
    "{'ES2002a': \n",
    "     ['okay',\n",
    "      'right',\n",
    "      'um',\n",
    "      'well',\n",
    "      ...\n",
    "     ],\n",
    " 'ES2002b':\n",
    "     [...],\n",
    "     ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(diction):\n",
    "    \"\"\"\n",
    "    Tokenize all the words in dictionary.\n",
    "    The tokenized pattern is defined with 'Regexp.Tokenizer'.\n",
    "    \n",
    "    Arguments:\n",
    "    diction -- the dictionary whose value should be tokenized\n",
    "    \n",
    "    Return:\n",
    "    w_dict -- token dictionary.\n",
    "    \"\"\"\n",
    "    w_dict = dict()\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "    for i in list(diction.keys()):\n",
    "        words = diction[i]\n",
    "        w_dict[i] = list(tokenizer.tokenize(words))\n",
    "    return w_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the words as tokens in dictionary.\n",
    "words_dict = tokenize_words(text_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removal\n",
    "Stop words are often functional words in english and they always be insignificant for text itself. Thus, we should remove it from the original tokens.\n",
    "\n",
    "#### - Read stop words text file\n",
    "Function `parse_one_text` is used to parse the stopwords file into a string. And then, use the `split(\\n)` function to split the string into list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopstr = parse_one_text('stopwords_en.txt')\n",
    "stopwords = stopstr.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Remove stop words\n",
    "* unique the words list and stopwords list\n",
    "* remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords)\n",
    "stopped_tokens = dict()\n",
    "for i in list(words_dict.keys()):\n",
    "    stopped_tokens[i] = [w for w in words_dict[i] if w not in stopwords_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Document Frequency Words Removal\n",
    "This part, we will remove all the tockens whose document frequency are greater than 132. Thus, we should find out which word has a high document frequency (> 132).  \n",
    "Function `chain.from_iterable` is used to concatenate all the tokenizers. Fucntion:\n",
    "``` python\n",
    "    list(chain.from_iterable([set(value) for value in stopped_tokens.values()]))\n",
    "```\n",
    "First, we get the values in each key of the pre-defined dictionaty and se the list of values into set() to make all the tokens into unique in each key. Then the `chain.from_iterable` function is used to chain all the unique tokens in one one key with iterate the dictionary keys. Finally, put the result into list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2 = list(chain.from_iterable([set(value) for value in stopped_tokens.values()]))\n",
    "word_freq = FreqDist(words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `FreqDist` in `nltk.book` calculate the token frequency. The output of this function is a dictionary type of `word : frequency`. Example:\n",
    "``` json\n",
    "        { 'cheery': 1,\n",
    "          'ironic': 1,\n",
    "          'thinking': 94,\n",
    "          'product': 110,\n",
    "          ...\n",
    "        }\n",
    "```\n",
    "Filter the words with it's frequency. Store the tokens in a list with its frequency not greater than 132 and put the selected results into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_1 = []\n",
    "for i in list(word_freq):\n",
    "    if word_freq[i] <= 132:\n",
    "        tokens_1.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len() < 3 Words Removal\n",
    "The token length may be smaller than 3. However, most of the words with its length smaller than 3 would be insignificance such as 'hu', 'a_' and etc. Thus, in this step, we will delete all the tokens with its length smaller than 3. Then, we sort the final token with its alphabetic order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [i for i in tokens_1 if len(i) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort token in alphabetic order\n",
    "tokens.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 1: Write Tokens into `vocab.txt` File\n",
    "It contains the unigram vocabulary in the following format, word_string:integer_index. Words in the vocabulary must be sorted in alphabetic order. For example, \"absolute:22\" in the following figure means that the 23rd word in the vocabulary is \"absolute\".\n",
    "**************************\n",
    "'token : order number'\n",
    "\n",
    "The token and its order is generated by data structure dictionary with its token as key and order as value.\n",
    "\n",
    "Then read all the contains in this dictionary into a string with the structure of \n",
    "``` python\n",
    "    dict.key + ':' + str(dict.value) + '\\n'\n",
    "```\n",
    "Finally, write the final string into the `vocab.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dict (token_dic) of tokens\n",
    "token_dic = dict()\n",
    "for i in range(len(tokens)):\n",
    "    token_dic[tokens[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = ''\n",
    "for i in list(token_dic.keys()):\n",
    "    voc = voc + i + ':' + str(token_dic[i]) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = open('vocab.txt', 'w')\n",
    "vocab.write(voc)\n",
    "vocab.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 2: Generate `topic_segs.txt` file\n",
    "It contains the topic boundaries encoded in boolean vectors. For example, if a meeting transcript, \"ES2018d.txt\" contains 10 paragraphs in total after being preprocessed, and there are topic boundaries after the 2nd, 5th, and 7th paragraphs, the boolean vector must be \"__ES2018d:0,1,0,0,1,0,1,0,0,1__\". Every line in *topic_segs.txt* corresponds to one meeting transcript.\n",
    "*******************\n",
    "For this task, the directory `text_dict` will be used.\n",
    "\n",
    "Get a string object to record all the segment info with 0 (represent a normal paragraph) and 1 (represent the last paragraph in each topic). Each line records each meeting transcript file. \n",
    "\n",
    "First, we generate a list to record 0 and 1 features of each meeting transcript file. And then `join` function is used to connect all the value in list into a string and seperate each item by ','. Finally, add every string of each meeting transcript file into a total string by sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a string to record segment info\n",
    "seg_str = ''\n",
    "for i in list(text_dict.keys()):\n",
    "    sub_ls = []\n",
    "    sub_str = i + ':'\n",
    "    str_ls = text_dict[i].split('\\n')\n",
    "    for j in str_ls:\n",
    "        if j == '**********':\n",
    "            sub_ls[-1] = '1'\n",
    "        elif j == '':\n",
    "            continue\n",
    "        else:\n",
    "            sub_ls.append('0')\n",
    "    sub_str = sub_str + ','.join(sub_ls) + '\\n'\n",
    "    seg_str = seg_str + sub_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string object is generated before. The next task is to write the string into text file: `topic_segs.txt`.\n",
    "\n",
    "For this task, it's not clear about write the final result in which file. Because, in assignment specification, it said write it into `topic_seg.txt` file. However, the zip file provide us an empty `topic_segs.txt` file. Thus, I generate the two files and write the same result into them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_seg = open('topic_segs.txt', 'w')\n",
    "topic_seg.write(seg_str)\n",
    "topic_seg.close()\n",
    "topic_seg = open('topic_seg.txt', 'w')\n",
    "topic_seg.write(seg_str)\n",
    "topic_seg.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 3: Generate `./sparse_files/*.txt` File\n",
    "Each txt file in the \"sparse_files\" folder corresponds to one of the meeting transcripts in the \"txt_files\" folder, and they have the same file name.  For example, \"./sparse_files/ES2002a.txt\" corresponds to \"./txt_files/ES2002a.txt\". Each file in \"/sparse_files\" contains the sparse representations for all its paragraphs.\n",
    "************************\n",
    "The funtion `write_sparse` is used to write the sparsed result into a text file.\n",
    "\n",
    "The function `sparse_one_line` is used to sparse only one line in all of the meeting transcript text files. In each line, we should justify if the word is in the token dictionary `token_dic` which is genarated before. If yes, we should record the index of the word and its frequency in each line. The resord structure would be:\n",
    "``` python\n",
    "    str(token_dic[a_key]) + ':' + str(w_freq[key])\n",
    "```\n",
    "It should be notice that the word might represent as `'kay`. However, in previous token extraction, we only extract the words like `kay`. Thus, when we got a token in non-extrated word token dictionary, the token should still contians this kind of symbol like `'kay`. It would be match in the normal token dictionary. Thus, before to get the index of each word, we shoud use regular expression `re.findall(\"\\w+(?:[-']\\w+)?\", i)[0]` to select the useful word part. Finally, connect all the word index and its frequency into a single string by sequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sparse(string, file_path):\n",
    "    \"\"\"\n",
    "    Write the sparsed string into a text file with defined path.\n",
    "    \n",
    "    Arguments:\n",
    "    string -- input string which need to be write into text file\n",
    "    file_path -- the path of the text to write\n",
    "    \n",
    "    Return:\n",
    "    None\n",
    "    \"\"\"\n",
    "    text_file = open(file_path, 'w')\n",
    "    text_file.write(string)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_one_line(line, token_dic):\n",
    "    \"\"\"\n",
    "    Sparse one line of all the topic text file by selecting its the useful words.\n",
    "    And the frequency of the word in this line.\n",
    "    \n",
    "    Arguments:\n",
    "    line -- a string line with words.\n",
    "    token_dic -- the token dictionary to record the token and its index.\n",
    "    \n",
    "    Return:\n",
    "    sparse_str -- the string chained by line result\n",
    "    \"\"\"\n",
    "    ls = []\n",
    "    ws = line.split()\n",
    "    for i in ws:\n",
    "        for j in re.findall(\"\\w+(?:[-']\\w+)?\", i):\n",
    "            try:\n",
    "                ls.append(token_dic[j])\n",
    "            except:\n",
    "                continue\n",
    "    w_freq = FreqDist(ls)\n",
    "    l_ls = []\n",
    "    for k in w_freq.keys():\n",
    "        l_ls.append(str(k) + ':' + str(w_freq[k]))\n",
    "    sparse_str = ','.join(l_ls)\n",
    "    if len(sparse_str) != 0:\n",
    "        sparse_str = sparse_str + '\\n'\n",
    "    return sparse_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all the keys in the text dictionary which records the file name and the file contianing as string. For each text dictionary value, `split('\\n')` is used to split the string file into list lines. \n",
    "\n",
    "And then retrieve its lines and parse each line using the pre-defined function `sparse_one_line()` and connect line results together. \n",
    "\n",
    "After sparsing each file, the result will be wrote into a text file and store the file into `.\\sparse_files` folder with the function `write_sparse()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(text_dict.keys()):   # file name will be i.txt\n",
    "    topic = ''\n",
    "    str_ls = text_dict[i].split('\\n')  # split lines in each topic file\n",
    "    for j in range(len(str_ls)): \n",
    "        topic = topic + sparse_one_line(str_ls[j], token_dic) # parse each line\n",
    "    spr_name = i + '.txt'\n",
    "    sps_path = os.path.join(sparse_path, spr_name)\n",
    "    write_sparse(topic, sps_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* learned how to select word tokens. From the first sub-task, we understand what is stop words and what kind of words should be selected as tokens. In general, when the word length is smaller than 3, the word might be nonsignificant. \n",
    "* Sparse representation represents the features of token in each paragragh.\n",
    "* The aim of text pre-processing is processing the text to get features of the text. The features may be the words the lines  or the paragraphs or even the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
